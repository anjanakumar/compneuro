%% set params

trials=80;
e=0.05;
b=1;

u=['A','B','C']; 
A=1;
B=2;
C=3;
m=ones(length(u),2)/2; % flat prior for actions
v=zeros(length(u),1);
w=v;

r=[0,2,5];
rewards=[0,5;2,0];


%% trial loop

%loggers

reward_log=nan(1,trials);
states_log=nan(1,trials);
m_log=nan(size(m,1),size(m,2),trials);
for i=1:trials
    
    %###
    % run in the maze
    
    if rand<m(A,1) % go left with probability b
        state=B;
        if rand<m(B,1) % go left with probability b_left
            reward=0;
        else
            reward=5;
        end
    else
        state=C;
        if rand<m(C,1) % go left with probability c_left
            reward=2;
        else
            reward=0;
        end
    end  % end maze
    %###
    
    % critic learning rule (equation 9.24)
    
    % delta = reward + v(u')-v(u)
    delta_A=reward +v(state)-v(A);
    delta_state=reward + reward - v(state);
    
    % w = w+ed
    
    w(A)= w(A)+e*delta_A;
    w(state)=w(state)+e*delta_state;
    
    % actor learning rule (equation 9.25)
    
    delta_A_L=reward+w(B)-w(A);
    delta_A_R=reward+w(C)-w(A);
    
    % m(u)= m(u)+e(delta_aa-P(a|u))*delta
    % P(a|u)=softmax(m(state_,:));
    m(A,:)=softmax(m(A,:)+e*([delta_A_L,delta_A_R]-softmax(m(A,:)))*delta_A);
    m(state,:)=softmax(m(state,:)+e*(rewards(state-1,:)-softmax(m(state,:)))*delta_state);
    
    % loggers
    states_log(i)=state;
    reward_log(i)=reward;
    m_log(:,:,i)=m;
    
end

%% output

figure(1);
plot(reward_log)
title('Rewards received by agent.')
xlabel('trial')
ylabel('reward')

figure(2);
hist(states_log)
title('Behavior statistics');
xlabel('B=2, C=3')
ylabel('Times the agent visited the state')
xlim([1.5 3.5]);

figure(3)
hold off
plot(squeeze(m_log(1,1,:)))
hold on
plot(squeeze(m_log(1,2,:)))
plot(squeeze(m_log(2,1,:)))
plot(squeeze(m_log(2,2,:)))
plot(squeeze(m_log(3,1,:)))
plot(squeeze(m_log(3,2,:)))
legend('A left','A right','B left','B right','C left','C right')
title('Decision probabilities')
ylabel('Probability')
xlabel('trial')

disp(['Average reward was ' num2str(sum(reward_log)/trials)]);

if sum(reward_log)/trials<2.1
    disp('The agent was too greedy to explore effectively.');
elseif any(3==states_log)
    disp('The agent explored and found the big reward');
else
    disp('The agent was greedy, but at least it ended up on the big reward')
end;
