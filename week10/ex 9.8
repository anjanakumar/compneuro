%% set params

trials=10;
e=0.5;
b=1;

u=['A','B','C'];
A=1;
B=2;
C=3;
m=ones(length(u),2)/2; % flat prior for actions
v=zeros(length(u),1);
w=v;

r=[0,2,5];
rewards=[0,5;2,0];


%%

%loggers

reward_log=nan(1,trials);
for i=1:trials
    
    %###
    % run in the maze
    
    if rand<m(A,1) % go left with probability b
        state=B;
        if rand<m(B,1) % go left with probability b_left
            reward=0;
        else
            reward=5;
        end
    else
        state=C;
        if rand<m(C,1) % go left with probability c_left
            reward=2;
        else
            reward=0;
        end
    end        
    %###
    
    % critic learning rule (equation 9.24)
    
    % delta = reward + v(u')-v(u)
    delta_A=reward +v(state)-v(A);
    delta_state=reward + reward - v(state);
    
    % w = w+ed
    
    w(A)= w(A)+e*delta_A;
    w(state)=w(state)+e*delta_state;
    
    
    
    % actor learning rule (equation 9.25)
    delta_A_L=reward+w(B)-w(A);
    delta_A_R=reward+w(C)-w(A);
    % m(u)= m(u)+e(delta_aa-P(a|u))*delta
    % P(a|u)=softmax(m(state_,:));
    m(A,:)=m(A,:)+e*([delta_A_L,delta_A_R]-softmax(m(A,:)))*delta_A;
    m(B,:)=m(B,:)+e*(rewards(state-1,:)-softmax(m(state,:)))*delta_state;
    
    % loggers
    reward_log(i)=reward;
    
end

plot(reward_log)
tilte('sorry for this bad reward plot, I was out of time.')

