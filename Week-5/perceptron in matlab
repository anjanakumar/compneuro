function [percCorrect,conv]=PercyGD(maxiter,eta,nop,nod,momentum,stochastic)

% maxiter     : Maximum number of iterations if GD doesn't converge.
% eta         : Learning step size. 
% nop         : Number of patterns / pieces of data
% nod         : Number of dimensions / neurons
% momentum    : 1 to apply momentum, 0 to use normal GD learning. 
% stochastic  : 1 to use stochastic GD, 0 to use batch GD.

% example call:
%[percCorr,conv]=PercyGD(1000,0.0001,20,10,1,0) 
%%
% init

binarySequence=@(n) round(rand(1,n)); % returns binary pattern of lenght n

% init xi
xi=nan(nod,nop);

for i=1:nop
    % fill xi with patterns
    xi(:,i)=binarySequence(nod);
end

z=binarySequence(nop); % init labels

w=rand(1,nod)-0.5; % init weights around 0

%% perceptron loop
tempcount=0;
% init params

iter=0; 
dw=rand(1,nod); % init as random to pass through the while conditions
old_dw=zeros(1,nod); % used if momentum = 1
a=0.5; % momentum coefficient

% -- train w --

while( (sum(dw)~=0) && (iter<maxiter)) % runs until max iters or until convergence
    
    iter=iter+1;
    if(stochastic)
        for nop_i = 1:nop % loop over patterns
            
            % calculate gradient
            % error func: sum [ ( wxi(i) - z(i) )^2 ] dw
            % derivative with respect to w:
            dw=(xi(:,nop_i).*(w*xi(:,nop_i)-z(nop_i)))';
            % can be thought of as pattern * (classification - label)
            
            % ! update w for every pattern
            if(momentum)
                
                % this actually amounts for the full momentum because on every
                % iteration it carries over some of the momentum of all the
                % previous dw.
                dw=dw+a*old_dw; % old_dw = 0 on the first iteration
                
            end
            
            % update weights
            w=w-eta*dw;  
            old_dw=dw; % used to calculate momentum
         end
            
    else    % run batch GD
        
        for nop_i = 1:nop % iterate over all patterns
        
            % derivative with respect to w
            dw=(xi(:,nop_i).*(w*xi(:,nop_i)-z(nop_i)))'; 
               
            if(momentum)
            dw=dw+a*old_dw; 
            end  
        end
        w=w-eta*dw; % weights get updated for all patterns cumilatively
        old_dw=dw;
    end
    
end

% --

% output convergence
conv = sum(w*xi);

%% classification

% classify wxi to 0 or 1. 
classif=w*xi>0.5; % If (i) > 0.5 classif(i)=1. 

% compare labels z with the perceptron classiffications classif
comparison=z==classif; % if z(i)=classif(i),comparison(i)=1, else 0.
% output performance
percCorrect=sum(comparison)/nop;
